root@hw:/home/denis# docker cp covid-data.csv denis_spark_1:/opt/bitnami/spark/covid-data.csv
root@hw:/home/denis# docker exec -it denis_spark_1 bash
I have no name!@c5cc41e6c0f5:/opt/bitnami/spark$ pyspark
>>> from pyspark.sql import SparkSession
>>> from pyspark.sql.functions import col, max as max_, lag, when, desc, asc
>>> from pyspark.sql.window import Window
>>> spark = SparkSession.builder \
...     .appName("COVID-19 Data Analysis") \
...     .getOrCreate()
>>> df = spark.read.csv("covid-data.csv", header=True, inferSchema=True)
>>> non_countries = ["World", "Europe", "European Union", "International",
...                 "Africa", "Asia", "North America", "South America",
...                 "Oceania", "High income", "Low income"]
>>> task1_df = (df.filter((col("date") == "2021-03-31") &
...                       ~col("location").isin(non_countries))
...     .select("iso_code", "location", "total_cases_per_million")
...     .withColumn("percent_infected", col("total_cases_per_million") / 10_000)
...     .orderBy(desc("percent_infected"))
...     .limit(15)
... )
>>> task1_df.write.csv("/tmp/output/task1", header=True, mode="overwrite")
>>> task1_result = spark.read.csv("/tmp/output/task1", header=True)
>>> print("=== Задание 1: 15 стран с наибольшим процентом переболевших ===")
=== Задание 1: 15 стран с наибольшим процентом переболевших ===
>>> task1_result.show(15, truncate=False)
+--------+-------------+-----------------------+------------------+
|iso_code|location     |total_cases_per_million|percent_infected  |
+--------+-------------+-----------------------+------------------+
|AND     |Andorra      |155439.073             |15.5439073        |
|MNE     |Montenegro   |145237.254             |14.523725399999998|
|CZE     |Czechia      |143088.484             |14.3088484        |
|SMR     |San Marino   |139371.796             |13.9371796        |
|SVN     |Slovenia     |103708.058             |10.370805800000001|
|LUX     |Luxembourg   |98473.424              |9.8473424         |
|ISR     |Israel       |96251.06               |9.625106          |
|USA     |United States|92030.11               |9.203011          |
|SRB     |Serbia       |88263.286              |8.8263286         |
|BHR     |Bahrain      |84888.601              |8.4888601         |
|PAN     |Panama       |82287.391              |8.2287391         |
|PRT     |Portugal     |80586.997              |8.0586997         |
|EST     |Estonia      |80226.816              |8.0226816         |
|SWE     |Sweden       |79697.443              |7.9697443         |
|LTU     |Lithuania    |79388.647              |7.9388647         |
+--------+-------------+-----------------------+------------------+
>>> non_countries = ["World", "Europe", "European Union", "International",
...                 "North America", "South America", "Asia", "Africa",
...                 "Oceania", "High income", "Low income", "Lower middle income",
...                 "Upper middle income"]
>>> task2_df = (df.filter((col("date").between("2021-03-25", "2021-03-31")) &
...              ~col("location").isin(non_countries))
)...     .groupBy("location")
...     .agg(max_("new_cases").alias("max_new_cases"))
...     .orderBy(desc("max_new_cases"))
...     .limit(10)
...     .select("location", "max_new_cases")
... )
>>> task2_df.write.csv("/tmp/output/task2_filtered", header=True, mode="overwrite")
>>> task2_result.show(10, truncate=False)
+-------------+-------------+
|location     |max_new_cases|
+-------------+-------------+
|Brazil       |100158.0     |
|United States|77321.0      |
|India        |72330.0      |
|France       |59054.0      |
|Turkey       |39302.0      |
|Poland       |35145.0      |
|Germany      |25014.0      |
|Italy        |24076.0      |
|Peru         |19206.0      |
|Ukraine      |18226.0      |
+-------------+-------------+
>>> task3_df = (df.filter((col("location") == "Russia") &
...                      (col("date").between("2021-03-25", "2021-03-31")))
...     .select("date", "new_cases")
...     .withColumn("prev_day_cases", lag("new_cases").over(Window.orderBy("date")))
...     .withColumn("delta", col("new_cases") - col("prev_day_cases"))
... )
>>> task3_df.write.csv("/tmp/output/task3", header=True, mode="overwrite")
25/08/04 11:10:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/08/04 11:10:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/08/04 11:10:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
>>> spark.sparkContext.setLogLevel("ERROR")
>>> task3_df.write.csv("/tmp/output/task3", header=True, mode="overwrite")
>>> task3_result = spark.read.csv("/tmp/output/task3", header=True)
>>> print("\n=== Задание 3: Изменение случаев в России ===")

=== Задание 3: Изменение случаев в России ===
>>> task3_result.show(7, truncate=False)  # 7 дней последней недели марта
+----------+---------+--------------+------+
|date      |new_cases|prev_day_cases|delta |
+----------+---------+--------------+------+
|2021-03-25|9128.0   |null          |null  |
|2021-03-26|9073.0   |9128.0        |-55.0 |
|2021-03-27|8783.0   |9073.0        |-290.0|
|2021-03-28|8979.0   |8783.0        |196.0 |
|2021-03-29|8589.0   |8979.0        |-390.0|
|2021-03-30|8162.0   |8589.0        |-427.0|
|2021-03-31|8156.0   |8162.0        |-6.0  |
+----------+---------+--------------+------+

>>>
